{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4"
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# SPARQL generation with pre-trained GPT for KG Question Answering"
      ],
      "metadata": {
        "id": "Ybl6lHKvweIf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Data processing and Libraries"
      ],
      "metadata": {
        "id": "S0VfgKcNTPrR"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "uAloa17_Rbdq",
        "outputId": "b2cd9fc3-94d3-445b-f5a4-4cca53fc360b"
      },
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "!gdown 1sl_YdyiucWmk8Lx2x-Qn5ALcr2bLFUb0\n",
        "!unzip DBLP-QuAD.zip"
      ],
      "metadata": {
        "id": "zXjIkCLjTFAk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "%%capture\n",
        "import json\n",
        "import requests\n",
        "import re\n",
        "import pickle\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn.modules.module import T\n",
        "from random import shuffle\n",
        "\n",
        "torch.manual_seed(1706)\n",
        "\n",
        "def repl_func(match):\n",
        "    return match.group(1).lower()\n",
        "\n",
        "def get_entities(question, label_generator=\"t5-small\", embedding_reranker=\"distmult\"):\n",
        "    base_url = \"https://ltdemos.informatik.uni-hamburg.de/dblplinkapi/api/entitylinker\"\n",
        "    endpoint_url = f\"{base_url}/{label_generator}/{embedding_reranker}\"\n",
        "    payload = {\"question\": question}\n",
        "    headers = {\"Content-Type\": \"application/json\"}\n",
        "    response = requests.post(endpoint_url, data=json.dumps(payload), headers=headers)\n",
        "    if response.status_code == 200:\n",
        "        return response.json()\n",
        "    else:\n",
        "        print(f\"Error {response.status_code}: {response.text}\")\n",
        "        return None\n",
        "\n",
        "def process_question(entities, question):\n",
        "  new_entities = get_entities(question)[\"entitylinkingresults\"]\n",
        "  found = 0\n",
        "  for entity in new_entities:\n",
        "    label = entity[\"label\"]\n",
        "    if label:\n",
        "      if \": \" in label: label = label.split(\": \")[1] # prune authors\n",
        "      if \". (\" in label: label = label.split(\". (\")[0] # prune year\n",
        "      if label[-1] == \".\": label = label[:-1] # prune last dot\n",
        "      if  (type(entity[\"result\"]) is list and entity[\"result\"] and\n",
        "          type(entity[\"result\"][0]) is list and len(entity[\"result\"][0]) > 1 and\n",
        "          type(entity[\"result\"][0][1]) is list and entity[\"result\"][0][1]):\n",
        "        iri = entity[\"result\"][0][1][0]\n",
        "        if (\"'\" + label + \"'\" in question) and (iri in entities):\n",
        "          found += 1\n",
        "          question = question.replace(\"'\" + label + \"'\", iri)\n",
        "        elif (label in question) and (iri in entities):\n",
        "          found += 1\n",
        "          question = question.replace(label, iri)\n",
        "        elif (len(label.split(\" \")) == 2) and (iri in entities): # If name has given and last name, try 4 combinations\n",
        "          given, last = label.split(\" \")\n",
        "          if given and last:\n",
        "            if last + \", \" + given in question:\n",
        "              found += 1\n",
        "              question = question.replace(last + \", \" + given, iri)\n",
        "            elif last + \", \" + given[0] + \".\" in question:\n",
        "              found += 1\n",
        "              question = question.replace(last + \", \" + given[0] + \".\", iri)\n",
        "            elif given[0] + \".\" + last in question:\n",
        "              found += 1\n",
        "              question = question.replace(given[0] + \".\" + last, iri)\n",
        "            elif given + last[0] + \".\" in question:\n",
        "              found += 1\n",
        "              question = question.replace(given + last[0] + \".\", iri)\n",
        "\n",
        "  if found == len(entities): return question\n",
        "  else: return \"\"\n",
        "\n",
        "def format_question(question): # punctuation can damage the undestanding when attached to a word\n",
        "  question = re.sub(r'^(.)', repl_func, question)\n",
        "  if question[-1] == \".\": question = question[:-1] # eliminate end dot\n",
        "  question = question.replace(\"?\", \"\") # eliminate question mark\n",
        "  return question"
      ],
      "metadata": {
        "id": "hHRGdoxmZk25"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "questions = []\n",
        "archives = [\"DBLP-QuAD/train/questions.json\"] #\"DBLP-QuAD/valid/questions.json\", \"DBLP-QuAD/test/questions.json\"]\n",
        "for archive in archives:\n",
        "  with open(archive, 'r', encoding='utf-8') as file:\n",
        "    data = json.load(file)\n",
        "    index = 0\n",
        "    for entry in data[\"questions\"][4000:]:\n",
        "      print(index)\n",
        "      index += 1\n",
        "      if entry[\"template_id\"] != \"TP61\":\n",
        "        query = entry[\"query\"][\"sparql\"]\n",
        "\n",
        "        question = format_question(entry[\"question\"][\"string\"])\n",
        "        question = process_question(entry[\"entities\"], question)\n",
        "        if question:\n",
        "          questions.append((question, query))\n",
        "\n",
        "        paraphrased = format_question(entry[\"paraphrased_question\"][\"string\"])\n",
        "        paraphrased = process_question(entry[\"entities\"], paraphrased)\n",
        "        if paraphrased:\n",
        "          questions.append((paraphrased, query))\n",
        "\n",
        "with open(\"/content/drive/MyDrive/DLBP-QuAD-train2.txt\", 'wb') as file:\n",
        "  pickle.dump(questions, file)\n",
        "print(len(questions))"
      ],
      "metadata": {
        "id": "5anHUyA8TWqz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "archives = [\"/content/drive/MyDrive/DLBP-QuAD-valid1.txt\", \"/content/drive/MyDrive/DLBP-QuAD-valid2.txt\",\n",
        "            \"/content/drive/MyDrive/DLBP-QuAD-test.txt\", \"/content/drive/MyDrive/DLBP-QuAD-train1.txt\",\n",
        "            \"/content/drive/MyDrive/DLBP-QuAD-train2.txt\"]\n",
        "questions = []\n",
        "for archive in archives:\n",
        "  with open(archive, 'rb') as file:\n",
        "    data = pickle.load(file)\n",
        "    questions += data"
      ],
      "metadata": {
        "id": "x6y1zbqsxirS"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "questions2 = []\n",
        "for w, q in questions:\n",
        "  if \"'\" not in w:\n",
        "    w = w.replace(\"(\", \"( \").replace(\")\", \" )\")\n",
        "    q = q.replace(\"(\", \"( \").replace(\")\", \" )\")\n",
        "    questions2.append((w, q))\n",
        "questions = questions2\n",
        "print(\"Size of new entity linked dataset:\", len(questions))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "GV3l57b-YrY0",
        "outputId": "1bc5022d-b7c0-4bf7-d26a-102a55f375f5"
      },
      "execution_count": 4,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of new entity linked dataset: 9289\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_entities = 0\n",
        "for q, a in questions:\n",
        "  for word in q:\n",
        "    if \"<\" in word: n_entities += 1\n",
        "print(\"Average number of entities per query:\", round(n_entities / len(questions), 3))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SWclkmETfmHl",
        "outputId": "e82ce8da-28b5-4a8e-9f83-66a90b010890"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Average number of entities per query: 1.231\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Encoding and decoding"
      ],
      "metadata": {
        "id": "ho5Acme5ul6F"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Encoder\n",
        "text = [a + \" \" + q for a, q in questions]\n",
        "words = list(set((\" \".join(text)).split(\" \")))\n",
        "words.append('¿') # Sequence init character\n",
        "words.append('¡') # Sequence end character\n",
        "words.append('<https://dblp.org/pid/27/4034-1>')\n",
        "words = sorted(words) # unique characters\n",
        "word_vocab_size = len(words) # amount of unique characters\n",
        "word_stoi = { w:i for i,w in enumerate(words) } # map char to int\n",
        "word_itos = { i:w for i,w in enumerate(words) } # map int to char\n",
        "word_encoder = lambda s: [word_stoi[w] for w in s.split(\" \")] # encoder\n",
        "word_decoder = lambda l: ' '.join([word_itos[i] for i in l]) # decoder\n",
        "print(\"Vocabulary size:\", len(words))"
      ],
      "metadata": {
        "id": "sHxKryviuoLj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "118eee12-d0cc-487a-bafa-60a5a30783a2"
      },
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Vocabulary size: 9342\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model"
      ],
      "metadata": {
        "id": "mN39KbQvRzz_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# hyperparams\n",
        "batch_size = 32\n",
        "max_iters = 6000\n",
        "text_sample = max_iters // 6\n",
        "eval_interval = 200\n",
        "learning_rate = 7e-4\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "eval_iters = 200\n",
        "n_embd = 128\n",
        "n_head = 8\n",
        "n_layer = 4\n",
        "\n",
        "class Head(nn.Module):\n",
        "    \"\"\" one head of self-attention \"\"\"\n",
        "\n",
        "    def __init__(self, head_size, b_size, masked=True):\n",
        "        super().__init__()\n",
        "        self.key = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
        "        self.query = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
        "        self.value = nn.Linear(n_embd, head_size, bias=False, device=device)\n",
        "        self.register_buffer('tril', torch.tril(torch.ones(b_size, b_size)))\n",
        "\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "        self.masked = masked\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "        B,T,C = x.shape\n",
        "        q = self.query(x) # (B,T,C)\n",
        "        if y is None:\n",
        "          k = self.key(x)   # (B,T,C)\n",
        "        else:\n",
        "          k = self.key(y)\n",
        "        wei = q @ k.transpose(-2,-1) * C**-0.5 # (B, T, C) @ (B, C, T) -> (B, T, T)\n",
        "        if self.masked:\n",
        "          mask = self.tril[:T, :T] == 0\n",
        "          wei = wei.masked_fill(mask.to(device), float('-inf')) # (B, T, T)\n",
        "        wei = F.softmax(wei, dim=-1) # (B, T, T)\n",
        "        wei = self.dropout(wei)\n",
        "        if y is None:\n",
        "          v = self.value(x) # (B,T,C)\n",
        "        else:\n",
        "          v = self.value(y)\n",
        "        out = wei @ v # (B, T, T) @ (B, T, C) -> (B, T, C)\n",
        "        return out\n",
        "\n",
        "class MultiHeadAttention(nn.Module):\n",
        "    \"\"\" multiple heads of self-attention in parallel \"\"\"\n",
        "    def __init__(self, num_heads, head_size, b_size, masked=True):\n",
        "        super().__init__()\n",
        "        self.heads = nn.ModuleList([Head(head_size, b_size, masked) for _ in range(num_heads)])\n",
        "        self.proj = nn.Linear(n_embd, n_embd, device=device)\n",
        "        self.dropout = nn.Dropout(dropout)\n",
        "\n",
        "    def forward(self, x, y=None):\n",
        "      if y is None:\n",
        "        out = torch.cat([h(x) for h in self.heads], dim=-1)\n",
        "      else:\n",
        "        out = torch.cat([h(x, y) for h in self.heads], dim=-1)\n",
        "      out = self.dropout(self.proj(out))\n",
        "      return out\n",
        "\n",
        "class FeedFoward(nn.Module):\n",
        "    \"\"\" a simple linear layer followed by a non-linearity \"\"\"\n",
        "    def __init__(self, n_embd):\n",
        "        super().__init__()\n",
        "        self.net = nn.Sequential(\n",
        "            nn.Linear(n_embd, 4 * n_embd, device=device),\n",
        "            nn.ReLU(),\n",
        "            nn.Linear(4 * n_embd, n_embd, device=device),\n",
        "            nn.Dropout(dropout),\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.net(x)\n",
        "\n",
        "class DecoderBlock(nn.Module):\n",
        "    \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "    def __init__(self, n_embd, n_head):\n",
        "        super().__init__()\n",
        "        head_size = n_embd // n_head\n",
        "\n",
        "        self.sa_decoder1 = MultiHeadAttention(n_head, head_size, MAX_OUT_BLOCK, masked=True)\n",
        "        self.sa_decoder2 = MultiHeadAttention(n_head, head_size, MAX_OUT_BLOCK, masked=False)\n",
        "        self.ffwd_decoder = FeedFoward(n_embd)\n",
        "        self.ln_decoder1 = nn.LayerNorm(n_embd, device=device)\n",
        "        self.ln_decoder2 = nn.LayerNorm(n_embd, device=device)\n",
        "        self.ln_decoder3 = nn.LayerNorm(n_embd, device=device)\n",
        "\n",
        "    def forward(self, x, y):\n",
        "        x = x + self.sa_decoder1(self.ln_decoder1(x))\n",
        "        x = x + self.sa_decoder2(self.ln_decoder2(x), y)\n",
        "        x = x + self.ffwd_decoder(self.ln_decoder3(x))\n",
        "        return x\n",
        "\n",
        "class EncoderBlock(nn.Module):\n",
        "  \"\"\" Transformer block: communication followed by computation \"\"\"\n",
        "  def __init__(self, n_embd, n_head):\n",
        "      super().__init__()\n",
        "      head_size = n_embd // n_head\n",
        "\n",
        "      self.sa_encoder = MultiHeadAttention(n_head, head_size, MAX_IN_BLOCK, masked=False)\n",
        "      self.ffwd_encoder = FeedFoward(n_embd)\n",
        "      self.ln_encoder1 = nn.LayerNorm(n_embd, device=device)\n",
        "      self.ln_encoder2 = nn.LayerNorm(n_embd, device=device)\n",
        "\n",
        "  def forward(self, x):\n",
        "      x = x + self.sa_encoder(self.ln_encoder1(x))\n",
        "      x = x + self.ffwd_encoder(self.ln_encoder2(x))\n",
        "      return x\n",
        "\n",
        "class TransformerModel(nn.Module):\n",
        "    def __init__(self, encoder_blocks, vocab_size):\n",
        "        super().__init__()\n",
        "        self.token_embedding_table = nn.Embedding(vocab_size, n_embd)\n",
        "        self.position_embedding_table = nn.Embedding(MAX_IN_BLOCK, n_embd)\n",
        "        self.position_embedding_table_dec = nn.Embedding(MAX_OUT_BLOCK, n_embd)\n",
        "        self.decoder_blocks = [DecoderBlock(n_embd, n_head) for _ in range(n_layer)]\n",
        "        self.encoder_blocks = encoder_blocks\n",
        "        self.ln_f = nn.LayerNorm(n_embd, device=device) # final layer norm\n",
        "        self.lm_head = nn.Linear(n_embd, vocab_size, device=device)\n",
        "\n",
        "    def forward(self, idx, idy, targets=None):\n",
        "        Bx, Tx = idx.shape\n",
        "        By, Ty = idy.shape\n",
        "        # Encoder\n",
        "        tok_emb_enc = self.token_embedding_table(idx) # (B,T,C)\n",
        "        pos_emb_enc = self.position_embedding_table(torch.arange(Tx, device=device)) # (T,C)\n",
        "        enc_x = tok_emb_enc + pos_emb_enc # (B,T,C)\n",
        "        enc_x = self.encoder_blocks(enc_x) # (B,T,C)\n",
        "        # Decoder\n",
        "        tok_emb = self.token_embedding_table(idy) # (B,T,C)\n",
        "        pos_emb = self.position_embedding_table_dec(torch.arange(Ty, device=device)) # (T,C)\n",
        "        x = tok_emb + pos_emb # (B,T,C)\n",
        "        for decoder_block in self.decoder_blocks:\n",
        "          x = decoder_block(x, enc_x) # (B,T,C)\n",
        "        x = self.ln_f(x) # (B,T,C)\n",
        "        logits = self.lm_head(x) # (B,T,vocab_size)\n",
        "\n",
        "        if targets is None:\n",
        "          loss = None\n",
        "        else:\n",
        "          B, T, C = logits.shape\n",
        "          logits = logits.view(B*T, C)\n",
        "          targets = targets.view(B*T)\n",
        "          loss = F.cross_entropy(logits, targets)\n",
        "        return logits, loss\n",
        "\n",
        "    def generate(self, idx, start_token_idx, max_new_tokens):\n",
        "      idy = torch.full((idx.size(0), 1), start_token_idx[0], dtype=torch.long, device=idx.device)\n",
        "\n",
        "      for _ in range(max_new_tokens):\n",
        "        # get the predictions\n",
        "        logits, _ = self(idx, idy)\n",
        "        # focus only on the last time step\n",
        "        logits = logits[:, -1, :] # becomes (B, C)\n",
        "        # apply softmax to get probabilities\n",
        "        probs = F.softmax(logits, dim=-1) # (B, C)\n",
        "        # sample from the distribution\n",
        "        idy_next = torch.multinomial(probs, num_samples=1) # (B, 1)\n",
        "        # append sampled index to the running sequence\n",
        "        idy = torch.cat((idy, idy_next), dim=1) # (B, T+1)\n",
        "      return idy\n",
        "\n",
        "    def save(self, path, optimizer):\n",
        "      torch.save({\n",
        "          'state_dict': self.state_dict(),\n",
        "          'optimizer': optimizer.state_dict()\n",
        "      }, path)\n",
        "\n",
        "    def load(self, checkpoint_path, optimizer):\n",
        "      checkpoint = torch.load(checkpoint_path)\n",
        "      self.load_state_dict(checkpoint['state_dict'])\n",
        "      optimizer.load_state_dict(checkpoint['optimizer'])"
      ],
      "metadata": {
        "id": "jDQ_sp1MmAXw"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Split data and loss estimation"
      ],
      "metadata": {
        "id": "aonNM0sL440h"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def split_and_encode_text(input_array, encoder):\n",
        "  result = []\n",
        "  max_in_block = -1\n",
        "  max_out_block = -1\n",
        "  shuffle(input_array)\n",
        "  for item in input_array:\n",
        "    in_task = torch.tensor(encoder(item[0]), dtype=torch.long)\n",
        "    out_task = torch.tensor(encoder(item[1]), dtype=torch.long)\n",
        "    result.append((in_task, out_task))\n",
        "\n",
        "    len_in_block = len(in_task)\n",
        "    len_out_block = len(out_task)\n",
        "\n",
        "    if max_in_block < len_in_block: max_in_block = len_in_block\n",
        "    if max_out_block < len_out_block: max_out_block = len_out_block\n",
        "\n",
        "  return result, max_in_block + 1, max_out_block + 1"
      ],
      "metadata": {
        "id": "JZQ8Qbjr49u-"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# Split vectorized data\n",
        "data, MAX_IN_BLOCK, MAX_OUT_BLOCK = split_and_encode_text(questions, word_encoder)\n",
        "\n",
        "entities = []\n",
        "for w in words:\n",
        "  if '<' in w and len(w) > 1: entities.append(w)\n",
        "print(\"Pretrain data size:\",  len(entities))\n",
        "entities = [(\" \".join([e]*MAX_IN_BLOCK), \" \".join([e]*MAX_OUT_BLOCK)) for e in entities]\n",
        "\n",
        "data_pre, MAX_IN_BLOCK_pre, MAX_OUT_BLOCx_pre = split_and_encode_text(entities, word_encoder)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "C9roXVnVEyoc",
        "outputId": "e40f67b2-40fc-4126-941b-7e33bc433aa9"
      },
      "execution_count": 9,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Pretrain data size: 7144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "print(\"MAX_IN_BLOCK\", MAX_IN_BLOCK)\n",
        "print(\"MAX_OUT_BLOCK\", MAX_OUT_BLOCK)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "0-jibQ_B98pj",
        "outputId": "5546ff9a-c06a-4a5d-b108-9121a9ee0aae"
      },
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MAX_IN_BLOCK 34\n",
            "MAX_OUT_BLOCK 49\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Auxiliary train and test methods"
      ],
      "metadata": {
        "id": "9HBDU2sOuaQH"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "@torch.no_grad()\n",
        "def estimate_loss():\n",
        "  out = {}\n",
        "  m.eval()\n",
        "  for split in ['train', 'val']:\n",
        "    losses = torch.zeros(eval_iters)\n",
        "    for k in range(eval_iters):\n",
        "      X, Y, targets = get_batch(split, word_encoder)\n",
        "      logits, loss = m(X, Y, targets)\n",
        "      losses[k] = loss.item()\n",
        "    out[split] = losses.mean()\n",
        "  m.train()\n",
        "  return out\n",
        "\n",
        "def get_batch(split, encoder):\n",
        "    data = train_data if split == 'train' else val_data # elige un dataset dependiendo de la etapa\n",
        "    ix = torch.randint(len(data)-1, (batch_size,))\n",
        "    batch = [data[i] for i in ix]\n",
        "    start_token = torch.tensor(encoder('¿'))\n",
        "    end_token = torch.tensor(encoder('¡'))\n",
        "    x = torch.stack([F.pad(in_v, (0, MAX_IN_BLOCK - len(in_v)), value=1) for (in_v, _) in batch])\n",
        "    y = torch.stack([F.pad(torch.cat([start_token, out_v]), (0, MAX_OUT_BLOCK - len(out_v) - 1), value=1) for (_, out_v) in batch])\n",
        "    t = torch.stack([F.pad(torch.cat([out_v, end_token]), (0, MAX_OUT_BLOCK - len(out_v) - 1), value=1) for (_, out_v) in batch])\n",
        "    x, y, t = x.to(device), y.to(device), t.to(device)\n",
        "    return x, y, t\n",
        "\n",
        "def train_model(m, estimate_loss, optimizer, encoder, decoder):\n",
        "  intermediate_prints = []\n",
        "  for iter in range(max_iters):\n",
        "    # every once in a while evaluate the loss on train and val sets\n",
        "    if iter % eval_interval == 0 or iter == max_iters - 1:\n",
        "      losses = estimate_loss()\n",
        "      print(f\"step {iter}: train loss {losses['train']:.4f}, val loss {losses['val']:.4f}\")\n",
        "\n",
        "    # sample a batch of data\n",
        "    xb, yb, targets = get_batch('train', encoder)\n",
        "\n",
        "    # evaluate the loss\n",
        "    logits, loss = m(xb, yb, targets)\n",
        "    optimizer.zero_grad(set_to_none=True)\n",
        "    loss.backward()\n",
        "    optimizer.step()\n",
        "\n",
        "    if iter % text_sample == 0 or iter == max_iters - 1:\n",
        "      input_tokens = torch.zeros((1, 1), dtype=torch.long)\n",
        "      xb, yb, targets = get_batch('val', encoder)\n",
        "      m.eval()\n",
        "      intermediate_prints.append(decoder(m.generate(xb, encoder('¿'), MAX_OUT_BLOCK)[0].tolist()))\n",
        "      m.train()\n",
        "    del xb\n",
        "    del yb\n",
        "    del targets\n",
        "\n",
        "  return intermediate_prints\n",
        "\n",
        "def test_model(samples, m, encoder, decoder, test_data):\n",
        "  one_shot_hits = 0\n",
        "  three_shot_hits = 0\n",
        "  hamming_distance = 0\n",
        "  m.eval()\n",
        "  for i, test in enumerate(test_data[:samples]):\n",
        "    if i % 20 == 0: print(\"Sample: \", str(i))\n",
        "    in_t, out_t = test\n",
        "    n_tokens = len(out_t)\n",
        "    padded_text = F.pad(in_t, (0, MAX_IN_BLOCK - len(in_t)), value=1)\n",
        "\n",
        "    misses = 0\n",
        "    out_list = out_t.tolist()\n",
        "    shot = m.generate(padded_text.unsqueeze(0).to(device), encoder('¿'), n_tokens)[0].tolist()[1:]\n",
        "    for a,b in zip(shot, out_list):\n",
        "      if a != b: misses += 1\n",
        "\n",
        "    if misses:\n",
        "      hamming_distance += misses\n",
        "      hit = False\n",
        "      for _ in range(2):\n",
        "        shot = m.generate(padded_text.unsqueeze(0).to(device), encoder('¿'), n_tokens)[0].tolist()[1:]\n",
        "        if shot == out_list: hit = True\n",
        "      if hit: three_shot_hits += 1\n",
        "    else:\n",
        "      one_shot_hits += 1\n",
        "      three_shot_hits += 1\n",
        "\n",
        "\n",
        "  acc1 = round(one_shot_hits / samples, 5)\n",
        "  acc3 = round(three_shot_hits / samples, 5)\n",
        "  mhd = round(hamming_distance / samples, 5)\n",
        "  m.train()\n",
        "  return acc1, acc3, mhd"
      ],
      "metadata": {
        "id": "I1ti2yCIuZ0K"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model with pretrain"
      ],
      "metadata": {
        "id": "Qo1VQXN9xE1r"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Pretrain"
      ],
      "metadata": {
        "id": "jMHlz6sNxWCj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment hyperparams\n",
        "dropout = 0.01\n",
        "split = 1\n",
        "\n",
        "# Data division\n",
        "n_test = int(len(data_pre))\n",
        "test_data = data_pre\n",
        "print(\"Test cases train:\", n_test)\n",
        "\n",
        "n_train = int(len(data_pre))\n",
        "print(\"Train cases:\", n_train)\n",
        "train_data = data_pre\n",
        "val_data = data_pre\n",
        "print(\"Validation cases:\", len(val_data))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "BHRuAyLPG1Xn",
        "outputId": "2282658e-2f44-48e4-ab04-958ed9a32bf8"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test cases train: 7144\n",
            "Train cases: 7144\n",
            "Validation cases: 7144\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Model init\n",
        "encoder_blocks = nn.Sequential(*[EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
        "full_model = TransformerModel(encoder_blocks, word_vocab_size)\n",
        "m = full_model.to(device)\n",
        "\n",
        "# Print init loss and hyperparams\n",
        "x1, y1, targets = get_batch('train', word_encoder)\n",
        "print(\"Device:\", device)\n",
        "logits, loss = m(x1, y1, targets)\n",
        "print(logits.shape)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# Instanciar optimizador\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "id": "0J-nCaN2l6rN",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8a736ebe-f7d8-43c2-a6ca-5c2fe70fdbab"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "torch.Size([1568, 9342])\n",
            "Loss: tensor(9.3091, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "3.203326 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 13800\n",
        "text_sample = max_iters // 6\n",
        "intermediate_prints = train_model(m, estimate_loss, optimizer, word_encoder, word_decoder)\n",
        "\n",
        "for idx, intermediate_print in enumerate(intermediate_prints):\n",
        "  print(\"Sample {}:\\n\".format(idx), intermediate_print)\n",
        "\n",
        "m.save(\"/content/drive/MyDrive/transformer-pretrain-final.pth\", optimizer)"
      ],
      "metadata": {
        "id": "YrPxXbwP5l0c",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "8b18bebe-e87f-47da-ea8f-c87a786ae986"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 9.2913, val loss 9.3068\n",
            "step 200: train loss 8.6762, val loss 8.6893\n",
            "step 400: train loss 7.8344, val loss 7.8238\n",
            "step 600: train loss 6.7773, val loss 6.7697\n",
            "step 800: train loss 5.4527, val loss 5.4815\n",
            "step 1000: train loss 4.0305, val loss 4.0335\n",
            "step 1200: train loss 2.6602, val loss 2.6543\n",
            "step 1400: train loss 1.5939, val loss 1.6073\n",
            "step 1600: train loss 0.8976, val loss 0.9260\n",
            "step 1800: train loss 0.5072, val loss 0.5153\n",
            "step 2000: train loss 0.2476, val loss 0.2630\n",
            "step 2200: train loss 0.1433, val loss 0.1411\n",
            "step 2400: train loss 0.0773, val loss 0.0873\n",
            "step 2600: train loss 0.0531, val loss 0.0523\n",
            "step 2800: train loss 0.0387, val loss 0.0353\n",
            "step 3000: train loss 0.0238, val loss 0.0286\n",
            "step 3200: train loss 0.0198, val loss 0.0201\n",
            "step 3400: train loss 0.0160, val loss 0.0164\n",
            "step 3600: train loss 0.0137, val loss 0.0139\n",
            "step 3800: train loss 0.0117, val loss 0.0115\n",
            "step 4000: train loss 0.0098, val loss 0.0099\n",
            "step 4200: train loss 0.0086, val loss 0.0085\n",
            "step 4400: train loss 0.0076, val loss 0.0075\n",
            "step 4600: train loss 0.0066, val loss 0.0068\n",
            "step 4800: train loss 0.0059, val loss 0.0059\n",
            "step 5000: train loss 0.0052, val loss 0.0052\n",
            "step 5200: train loss 0.0047, val loss 0.0046\n",
            "step 5400: train loss 0.0041, val loss 0.0041\n",
            "step 5600: train loss 0.0037, val loss 0.0037\n",
            "step 5800: train loss 0.0033, val loss 0.0033\n",
            "step 6000: train loss 0.0029, val loss 0.0029\n",
            "step 6200: train loss 0.0026, val loss 0.0027\n",
            "step 6400: train loss 0.0024, val loss 0.0024\n",
            "step 6600: train loss 0.0021, val loss 0.0021\n",
            "step 6800: train loss 0.0019, val loss 0.0019\n",
            "step 7000: train loss 0.0017, val loss 0.0017\n",
            "step 7200: train loss 0.0015, val loss 0.0015\n",
            "step 7400: train loss 0.0014, val loss 0.0014\n",
            "step 7600: train loss 0.0012, val loss 0.0012\n",
            "step 7800: train loss 0.0011, val loss 0.0011\n",
            "step 8000: train loss 0.0010, val loss 0.0010\n",
            "step 8200: train loss 0.0009, val loss 0.0009\n",
            "step 8400: train loss 0.0008, val loss 0.0008\n",
            "step 8600: train loss 0.0007, val loss 0.0007\n",
            "step 8800: train loss 0.0007, val loss 0.0007\n",
            "step 9000: train loss 0.0006, val loss 0.0006\n",
            "step 9200: train loss 0.0005, val loss 0.0005\n",
            "step 9400: train loss 0.0005, val loss 0.0005\n",
            "step 9600: train loss 0.0004, val loss 0.0004\n",
            "step 9800: train loss 0.0004, val loss 0.0004\n",
            "step 10000: train loss 0.0004, val loss 0.0003\n",
            "step 10200: train loss 0.0003, val loss 0.0003\n",
            "step 10400: train loss 0.0003, val loss 0.0003\n",
            "step 10600: train loss 0.0003, val loss 0.0003\n",
            "step 10800: train loss 0.0002, val loss 0.0002\n",
            "step 11000: train loss 0.0002, val loss 0.0002\n",
            "step 11200: train loss 0.0002, val loss 0.0002\n",
            "step 11400: train loss 0.0002, val loss 0.0002\n",
            "step 11600: train loss 0.0002, val loss 0.0002\n",
            "step 11800: train loss 0.0001, val loss 0.0001\n",
            "step 12000: train loss 0.0001, val loss 0.0001\n",
            "step 12200: train loss 0.0001, val loss 0.0001\n",
            "step 12400: train loss 0.0001, val loss 0.0001\n",
            "step 12600: train loss 0.0001, val loss 0.0001\n",
            "step 12800: train loss 0.0001, val loss 0.0001\n",
            "step 13000: train loss 0.0001, val loss 0.0001\n",
            "step 13200: train loss 0.0001, val loss 0.0001\n",
            "step 13400: train loss 0.0001, val loss 0.0001\n",
            "step 13600: train loss 0.0001, val loss 0.0001\n",
            "step 13799: train loss 0.0000, val loss 0.0000\n",
            "Sample 0:\n",
            " ¿ <https://dblp.org/rec/conf/icebe/ZengLDCS06> <https://dblp.org/rec/conf/kes/Phillips-WrenI04> <https://dblp.org/rec/conf/isbi/WangY13> <https://dblp.org/pid/70/5760> 'Electronics <https://dblp.org/rec/conf/bdiot2/TamirCOR21> <https://dblp.org/pid/301/6480> <https://dblp.org/rec/conf/iecon/ChenLWDB17> <https://dblp.org/rec/journals/pr/DuffnerO14> <https://dblp.org/rec/conf/isbi/AndreVBWA10> <https://dblp.org/rec/journals/tcst/DaafouzGB98> <https://dblp.org/pid/94/4253> <https://dblp.org/rec/conf/icassp/SalamiHA89> 'FDL' Rep., <https://dblp.org/pid/m/TalMalkin> <https://dblp.org/pid/69/3087> 'COMPSAC <https://dblp.org/pid/10/6173> 'JAC' <https://dblp.org/rec/conf/coling/DaiA20> 'FDSE' <https://dblp.org/rec/journals/corr/abs-2101-06916> <https://dblp.org/pid/118/5591> <https://dblp.org/rec/journals/firstmonday/Kuijpers18> <https://dblp.org/rec/conf/amcc/BezzoF11> <https://dblp.org/rec/conf/icc/0017011> <https://dblp.org/rec/conf/iccchina/GuiZSW17> <https://dblp.org/pid/53/3719> <https://dblp.org/rec/conf/icpads/Al-SadiDO02> 'SSDBM' )-5 <https://dblp.org/pid/28/5745> <https://dblp.org/rec/conf/robio/ZhaoLC06> <https://dblp.org/pid/28/881> <https://dblp.org/pid/56/5639> <https://dblp.org/pid/52/4513> <https://dblp.org/pid/k/AngelaKunoth> Mon., Robots Short <https://dblp.org/pid/141/0898> <https://dblp.org/rec/conf/eucc/PeguerolesR19> <https://dblp.org/rec/conf/lopstr/BasinDFHN04>, <https://dblp.org/rec/journals/corr/abs-2102-08771> <https://dblp.org/rec/conf/icassp/LiuK04> <https://dblp.org/rec/conf/bdcloud/QiNXWY14>, AMIA ePrint\n",
            "Sample 1:\n",
            " ¿ <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97> <https://dblp.org/rec/journals/compsys/Ishida97>\n",
            "Sample 2:\n",
            " ¿ <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun> <https://dblp.org/pid/u/RaquelUrtasun>\n",
            "Sample 3:\n",
            " ¿ <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/rec/journals/tc/MajumderSPK12> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983> <https://dblp.org/pid/79/3983>\n",
            "Sample 4:\n",
            " ¿ <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15> <https://dblp.org/rec/conf/icpram/ArmanoT15>\n",
            "Sample 5:\n",
            " ¿ <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83> <https://dblp.org/rec/conf/ifip/CohenP83>\n",
            "Sample 6:\n",
            " ¿ <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250> <https://dblp.org/pid/116/1250>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc1, acc3, mhd = test_model(200, m, word_encoder, word_decoder, test_data)\n",
        "print('Accuracy@1 on test: {}%'.format(acc1 * 100))\n",
        "print('Accuracy@3 on test: {}%'.format(acc3 * 100))\n",
        "print('Mean Hamming distance on test: {}'.format(mhd))"
      ],
      "metadata": {
        "id": "nTfqLmKYrvug",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "4122d160-159d-4246-d501-cdb22d7dc4b0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample:  0\n",
            "Sample:  20\n",
            "Sample:  40\n",
            "Sample:  60\n",
            "Sample:  80\n",
            "Sample:  100\n",
            "Sample:  120\n",
            "Sample:  140\n",
            "Sample:  160\n",
            "Sample:  180\n",
            "Accuracy@1 on test: 100.0%\n",
            "Accuracy@3 on test: 100.0%\n",
            "Mean Hamming distance on test: 0.0\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "kCk-VWYexfWL"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Experiment hyperparams\n",
        "dropout = 0.01\n",
        "split = 0.95\n",
        "\n",
        "# Data division\n",
        "n_test = int(0.02*len(data))\n",
        "test_data = data[:n_test]\n",
        "print(\"Test cases train:\", n_test)\n",
        "\n",
        "data = data[n_test:]\n",
        "n_train = int(split*len(data))\n",
        "print(\"Train cases:\", n_train)\n",
        "train_data = data[:n_train]\n",
        "val_data = data[n_train:]\n",
        "print(\"Validation cases:\", len(val_data))"
      ],
      "metadata": {
        "id": "LAKfJtuUID81",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd1fa22c-aad2-4197-b9e3-7346428ee490"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Test cases train: 185\n",
            "Train cases: 8648\n",
            "Validation cases: 456\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 4200\n",
        "text_sample = max_iters // 6\n",
        "\n",
        "intermediate_prints = train_model(m, estimate_loss, optimizer, word_encoder, word_decoder)\n",
        "\n",
        "for idx, intermediate_print in enumerate(intermediate_prints):\n",
        "  print(\"Sample {}:\\n\".format(idx), intermediate_print)"
      ],
      "metadata": {
        "id": "lIIuPgGOJQ1B",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e52aa231-8659-4d2e-fd55-3ce3ecf8660a"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 13.9225, val loss 13.9496\n",
            "step 200: train loss 0.6193, val loss 0.6858\n",
            "step 400: train loss 0.3613, val loss 0.4745\n",
            "step 600: train loss 0.2206, val loss 0.3486\n",
            "step 800: train loss 0.1373, val loss 0.2721\n",
            "step 1000: train loss 0.0911, val loss 0.2368\n",
            "step 1200: train loss 0.0626, val loss 0.2139\n",
            "step 1400: train loss 0.0493, val loss 0.1904\n",
            "step 1600: train loss 0.0368, val loss 0.1796\n",
            "step 1800: train loss 0.0262, val loss 0.1821\n",
            "step 2000: train loss 0.0206, val loss 0.1654\n",
            "step 2200: train loss 0.0165, val loss 0.1570\n",
            "step 2400: train loss 0.0142, val loss 0.1542\n",
            "step 2600: train loss 0.0113, val loss 0.1537\n",
            "step 2800: train loss 0.0088, val loss 0.1512\n",
            "step 3000: train loss 0.0072, val loss 0.1528\n",
            "step 3200: train loss 0.0065, val loss 0.1537\n",
            "step 3400: train loss 0.0060, val loss 0.1553\n",
            "step 3600: train loss 0.0053, val loss 0.1586\n",
            "step 3800: train loss 0.0048, val loss 0.1459\n",
            "step 4000: train loss 0.0047, val loss 0.1548\n",
            "step 4199: train loss 0.0043, val loss 0.1540\n",
            "Sample 0:\n",
            " ¿ <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/pid/99/4092> '1986' other <https://dblp.org/pid/79/5272> <https://dblp.org/pid/79/5272> <https://dblp.org/pid/67/6858> <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/journals/jacm/Mason56> <https://dblp.org/pid/79/5272> <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/pid/24/6170> <https://dblp.org/rec/conf/embc/ParedesRCHRMFM11> <https://dblp.org/pid/274/7895> <https://dblp.org/rec/journals/tcs/MarriottO96> <https://dblp.org/pid/47/2026> <https://dblp.org/rec/conf/cvpr/ZhuMT03>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/iros/BerchtoldG94> <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/evoW/VilacaMRR10>, <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/rec/conf/eit/WangS18> <https://dblp.org/rec/conf/icmla/FahimG21> <https://dblp.org/rec/journals/tii/MadlPDA09> <https://dblp.org/pid/79/5272> <https://dblp.org/rec/conf/ccece/ZahirK13> <https://dblp.org/rec/conf/aime/GambergerL03> <https://dblp.org/pid/301/0408> <https://dblp.org/pid/79/5272>\n",
            "Sample 1:\n",
            " ¿ ASK { <https://dblp.org/pid/b/DanBoneh> <https://dblp.org/rdf/schema#authoredBy> ?x . <https://dblp.org/rec/conf/arc/ClausAAS10> <https://dblp.org/rdf/schema#authoredBy> ?x FILTER ( ?x . ?x . <https://dblp.org/rec/conf/worldcist/SantosS0CRCO20> != <https://dblp.org/rec/journals/access/LuoS20> } ¡ } ¡ ) } ¡ <https://dblp.org/rec/conf/worldcist/SantosS0CRCO20> <https://dblp.org/rdf/schema#authoredBy> ?x . != != != != != != != != != != != != != != != != != != != !=\n",
            "Sample 2:\n",
            " ¿ SELECT DISTINCT ?answer WHERE { <https://dblp.org/rec/journals/jcphy/SchmitzY12> <https://dblp.org/rdf/schema#numberOfCreators> ?x . ?x <https://dblp.org/rdf/schema#numberOfCreators> ?y > ?y, <https://dblp.org/rec/conf/rsctc/DohertyS04> <https://dblp.org/rdf/schema#numberOfCreators> ?answer ) AS ?answer ) AS ?answer ) AS ?count ) ) . ?x <https://dblp.org/rdf/schema#numberOfCreators> ?answer } GROUP BY ?y } ORDER BY ASC( ?count ) LIMIT 1 ¡ ) } ¡ <https://dblp.org/rdf/schema#numberOfCreators>\n",
            "Sample 3:\n",
            " ¿ ASK { <https://dblp.org/rec/conf/ht/RegattieriMM14> <https://dblp.org/rdf/schema#yearOfPublication> '2014' } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2014' } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2014' } ¡ University, <https://dblp.org/pid/156/0179> } ¡ } ¡ } ¡ } ¡ } } } ¡ } ¡ } ¡ } ¡ } } } ¡ } } } ¡ } ¡ } ¡ } ¡\n",
            "Sample 4:\n",
            " ¿ SELECT DISTINCT ?answer WHERE { <https://dblp.org/rec/journals/tcas/GaoJD19> <https://dblp.org/rdf/schema#yearOfPublication> ?answer } ¡ <https://dblp.org/rdf/schema#yearOfPublication> ?answer } ¡ . ?x <https://dblp.org/rdf/schema#yearOfPublication> ?answer } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡\n",
            "Sample 5:\n",
            " ¿ SELECT DISTINCT ?answer WHERE { { <https://dblp.org/rec/conf/smc/NumazawaN16> <https://dblp.org/rdf/schema#numberOfCreators> ?answer } UNION { <https://dblp.org/rec/conf/fdg/CabezasT13> <https://dblp.org/rdf/schema#numberOfCreators> ?answer } } ¡ <https://dblp.org/rdf/schema#numberOfCreators> ?answer } ¡ . ?answer } ¡ } } ¡ } } ¡ } } ¡ } ¡ } ¡ } } ¡ } ¡ } ¡ } ¡ }\n",
            "Sample 6:\n",
            " ¿ SELECT DISTINCT ?answer WHERE { <https://dblp.org/rec/conf/cvpr/GammeterGBQG10> <https://dblp.org/rdf/schema#yearOfPublication> ?answer } ¡ <https://dblp.org/rdf/schema#yearOfPublication> ?answer } ¡ . ?answer } ¡ } } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "m.save(\"/content/drive/MyDrive/transformer-pretrained-final.pth\", optimizer)"
      ],
      "metadata": {
        "id": "BMcWCDqZjq9Z"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "acc1, acc3, mhd = test_model(n_test, m, word_encoder, word_decoder, test_data)\n",
        "print('Accuracy@1 on test: {}%'.format(acc1 * 100))\n",
        "print('Accuracy@3 on test: {}%'.format(acc3 * 100))\n",
        "print('Mean Hamming distance on test: {}'.format(mhd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YCZUt4AmVIn1",
        "outputId": "5c82d6c1-02b7-4974-a65d-c2b217b961f0"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample:  0\n",
            "Sample:  20\n",
            "Sample:  40\n",
            "Sample:  60\n",
            "Sample:  80\n",
            "Sample:  100\n",
            "Sample:  120\n",
            "Sample:  140\n",
            "Sample:  160\n",
            "Sample:  180\n",
            "Accuracy@1 on test: 58.378%\n",
            "Accuracy@3 on test: 69.73%\n",
            "Mean Hamming distance on test: 1.49189\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Model without pretrain"
      ],
      "metadata": {
        "id": "VRQzgqrKI1Xz"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Train"
      ],
      "metadata": {
        "id": "Q8u8ScBFxjGK"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Model init\n",
        "encoder_blocks = nn.Sequential(*[EncoderBlock(n_embd, n_head) for _ in range(n_layer)])\n",
        "full_model = TransformerModel(encoder_blocks, word_vocab_size)\n",
        "m = full_model.to(device)\n",
        "\n",
        "# Print init loss and hyperparams\n",
        "x1, y1, targets = get_batch('train', word_encoder)\n",
        "print(\"Device:\", device)\n",
        "logits, loss = m(x1, y1, targets)\n",
        "print(logits.shape)\n",
        "print(\"Loss:\", loss)\n",
        "\n",
        "print(sum(p.numel() for p in m.parameters())/1e6, 'M parameters')\n",
        "\n",
        "# Instanciar optimizador\n",
        "optimizer = torch.optim.AdamW(m.parameters(), lr=learning_rate)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ElrdMLO01G29",
        "outputId": "ac3ac652-f353-41b5-82d4-9ef2ec738e98"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Device: cuda\n",
            "torch.Size([1568, 9342])\n",
            "Loss: tensor(8.8279, device='cuda:0', grad_fn=<NllLossBackward0>)\n",
            "3.203326 M parameters\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "max_iters = 18000\n",
        "text_sample = max_iters // 6\n",
        "intermediate_prints = train_model(m, estimate_loss, optimizer, word_encoder, word_decoder)\n",
        "\n",
        "for idx, intermediate_print in enumerate(intermediate_prints):\n",
        "  print(\"Sample {}:\\n\".format(idx), intermediate_print)\n",
        "\n",
        "m.save(\"/content/drive/MyDrive/transformer-final.pth\", optimizer)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NAC9SR_x1L9Y",
        "outputId": "fa8a8558-b03c-4a77-dfce-faeeca83341c"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "step 0: train loss 8.7867, val loss 8.7704\n",
            "step 200: train loss 1.1011, val loss 1.0806\n",
            "step 400: train loss 0.6660, val loss 0.6823\n",
            "step 600: train loss 0.4674, val loss 0.5228\n",
            "step 800: train loss 0.3737, val loss 0.4457\n",
            "step 1000: train loss 0.3300, val loss 0.4124\n",
            "step 1200: train loss 0.2889, val loss 0.3830\n",
            "step 1400: train loss 0.2642, val loss 0.3753\n",
            "step 1600: train loss 0.2409, val loss 0.3606\n",
            "step 1800: train loss 0.2195, val loss 0.3460\n",
            "step 2000: train loss 0.2066, val loss 0.3466\n",
            "step 2200: train loss 0.1883, val loss 0.3402\n",
            "step 2400: train loss 0.1735, val loss 0.3401\n",
            "step 2600: train loss 0.1626, val loss 0.3410\n",
            "step 2800: train loss 0.1437, val loss 0.3336\n",
            "step 3000: train loss 0.1278, val loss 0.3229\n",
            "step 3200: train loss 0.1087, val loss 0.3287\n",
            "step 3400: train loss 0.0973, val loss 0.3239\n",
            "step 3600: train loss 0.0815, val loss 0.3093\n",
            "step 3800: train loss 0.0685, val loss 0.3093\n",
            "step 4000: train loss 0.0565, val loss 0.3143\n",
            "step 4200: train loss 0.0449, val loss 0.2963\n",
            "step 4400: train loss 0.0350, val loss 0.2989\n",
            "step 4600: train loss 0.0258, val loss 0.2860\n",
            "step 4800: train loss 0.0217, val loss 0.2891\n",
            "step 5000: train loss 0.0166, val loss 0.2853\n",
            "step 5200: train loss 0.0132, val loss 0.2862\n",
            "step 5400: train loss 0.0103, val loss 0.2860\n",
            "step 5600: train loss 0.0086, val loss 0.2831\n",
            "step 5800: train loss 0.0064, val loss 0.2817\n",
            "step 6000: train loss 0.0051, val loss 0.2851\n",
            "step 6200: train loss 0.0051, val loss 0.2856\n",
            "step 6400: train loss 0.0049, val loss 0.2714\n",
            "step 6600: train loss 0.0043, val loss 0.2800\n",
            "step 6800: train loss 0.0055, val loss 0.2788\n",
            "step 7000: train loss 0.0032, val loss 0.2913\n",
            "step 7200: train loss 0.0036, val loss 0.2826\n",
            "step 7400: train loss 0.0028, val loss 0.2825\n",
            "step 7600: train loss 0.0025, val loss 0.2982\n",
            "step 7800: train loss 0.0046, val loss 0.2851\n",
            "step 8000: train loss 0.0033, val loss 0.2875\n",
            "step 8200: train loss 0.0037, val loss 0.2862\n",
            "step 8400: train loss 0.0034, val loss 0.2845\n",
            "step 8600: train loss 0.0021, val loss 0.2930\n",
            "step 8800: train loss 0.0015, val loss 0.2939\n",
            "step 9000: train loss 0.0012, val loss 0.2848\n",
            "step 9200: train loss 0.0072, val loss 0.2966\n",
            "step 9400: train loss 0.0019, val loss 0.2963\n",
            "step 9600: train loss 0.0011, val loss 0.3011\n",
            "step 9800: train loss 0.0013, val loss 0.2847\n",
            "step 10000: train loss 0.0044, val loss 0.2963\n",
            "step 10200: train loss 0.0019, val loss 0.2969\n",
            "step 10400: train loss 0.0018, val loss 0.3067\n",
            "step 10600: train loss 0.0010, val loss 0.2906\n",
            "step 10800: train loss 0.0013, val loss 0.3071\n",
            "step 11000: train loss 0.0021, val loss 0.2975\n",
            "step 11200: train loss 0.0017, val loss 0.2955\n",
            "step 11400: train loss 0.0016, val loss 0.2945\n",
            "step 11600: train loss 0.0017, val loss 0.2993\n",
            "step 11800: train loss 0.0041, val loss 0.3186\n",
            "step 12000: train loss 0.0009, val loss 0.2997\n",
            "step 12200: train loss 0.0005, val loss 0.3119\n",
            "step 12400: train loss 0.0005, val loss 0.2866\n",
            "step 12600: train loss 0.0002, val loss 0.2904\n",
            "step 12800: train loss 0.0002, val loss 0.2935\n",
            "step 13000: train loss 0.0006, val loss 0.2915\n",
            "step 13200: train loss 0.0107, val loss 0.3307\n",
            "step 13400: train loss 0.0027, val loss 0.2979\n",
            "step 13600: train loss 0.0009, val loss 0.2909\n",
            "step 13800: train loss 0.0005, val loss 0.2729\n",
            "step 14000: train loss 0.0002, val loss 0.2930\n",
            "step 14200: train loss 0.0003, val loss 0.2864\n",
            "step 14400: train loss 0.0004, val loss 0.2948\n",
            "step 14600: train loss 0.0004, val loss 0.2788\n",
            "step 14800: train loss 0.0004, val loss 0.2835\n",
            "step 15000: train loss 0.0037, val loss 0.3017\n",
            "step 15200: train loss 0.0026, val loss 0.3037\n",
            "step 15400: train loss 0.0009, val loss 0.2888\n",
            "step 15600: train loss 0.0008, val loss 0.2989\n",
            "step 15800: train loss 0.0004, val loss 0.2938\n",
            "step 16000: train loss 0.0006, val loss 0.2975\n",
            "step 16200: train loss 0.0003, val loss 0.2905\n",
            "step 16400: train loss 0.0002, val loss 0.2958\n",
            "step 16600: train loss 0.0002, val loss 0.2906\n",
            "step 16800: train loss 0.0004, val loss 0.2835\n",
            "step 17000: train loss 0.0064, val loss 0.2934\n",
            "step 17200: train loss 0.0028, val loss 0.3073\n",
            "step 17400: train loss 0.0013, val loss 0.2999\n",
            "step 17600: train loss 0.0006, val loss 0.2789\n",
            "step 17800: train loss 0.0004, val loss 0.2762\n",
            "step 17999: train loss 0.0003, val loss 0.2768\n",
            "Sample 0:\n",
            " ¿ <https://dblp.org/pid/r/RRajaraman> FOGA, <https://dblp.org/rec/journals/corr/abs-2201-11114> <https://dblp.org/pid/38/545> <https://dblp.org/pid/08/4002> <https://dblp.org/pid/286/9268> <https://dblp.org/pid/18/5058> <https://dblp.org/rec/journals/cas/SorrentiCV08> Datenbanken Computer-Aided <https://dblp.org/rec/conf/cdc/SepulchreSR10> Robotic <https://dblp.org/pid/18/1100> <https://dblp.org/rec/journals/corr/Hitzer13e> <https://dblp.org/rec/conf/aaai/Chan0GHSK22> <https://dblp.org/rec/journals/tcom/GhassemzadehJRTT04> <https://dblp.org/rec/journals/jwe/MaLXL15> <https://dblp.org/pid/36/3031> <https://dblp.org/rec/journals/ijcta/GuptaSZC14> <https://dblp.org/pid/37/3271> 2012 is <https://dblp.org/rec/journals/ijcomsys/JanicM06> <https://dblp.org/rec/conf/iske/LinXW08> <https://dblp.org/rec/journals/tjs/AksoyBYR21> Catalog' <https://dblp.org/rec/conf/vtc/TakedaA10> <https://dblp.org/rec/conf/ijcnn/CabritaRK10> <https://dblp.org/rec/conf/IEEEmsp/LeeCK02> <https://dblp.org/pid/58/9836> <https://dblp.org/rec/journals/vc/FanFCGL16> <https://dblp.org/rec/conf/fdg/CabezasT13> <https://dblp.org/pid/33/4000> <https://dblp.org/rec/journals/jbi/FilikovPKFO20> Supply GLOBECOM <https://dblp.org/rec/conf/premi/SharmaD19> <https://dblp.org/pid/53/35> T\\u00E9l\\u00E9communications, <https://dblp.org/rec/journals/entcs/OsorioC15> <https://dblp.org/rec/journals/ai/Thrun98> <https://dblp.org/rec/journals/taslp/GrootHDK11> <https://dblp.org/pid/26/4711> <https://dblp.org/rec/journals/sigpro/HuLGGY17> <https://dblp.org/rec/conf/icassp/MeillierC91> <https://dblp.org/pid/26/2519> <https://dblp.org/pid/47/3276> <https://dblp.org/pid/37/3003> <https://dblp.org/pid/s/KonstantinosFSagonas>\n",
            "Sample 1:\n",
            " ¿ SELECT ( COUNT( DISTINCT ?answer ) AS ?count ) WHERE { ?answer <https://dblp.org/rdf/schema#authoredBy> <https://dblp.org/rdf/schema#authoredBy> ?answer <https://dblp.org/rdf/schema#publishedIn> 'ASP-DAC' WHERE { ?answer <https://dblp.org/rdf/schema#publishedIn> 'Sensors' } ¡ <https://dblp.org/rdf/schema#publishedIn> ?answer <https://dblp.org/rdf/schema#publishedIn> 'CoRR' } ¡ <https://dblp.org/rdf/schema#publishedIn> ?answer } GROUP BY } ¡ } ¡ } ¡ } ¡ } ¡ } ¡ } ¡\n",
            "Sample 2:\n",
            " ¿ ASK { { <https://dblp.org/rdf/schema#yearOfPublication> '2018' } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2018' } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2018' } ¡ } } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2007' } } } } ¡ } } ¡ } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2018' } ¡ <https://dblp.org/rdf/schema#publishedIn> 'SC' } } ¡ } ¡ <https://dblp.org/rdf/schema#yearOfPublication> '2018' } ¡ } ¡ } ¡\n",
            "Sample 3:\n",
            " ¿ SELECT DISTINCT ?answer WHERE { ?answer <https://dblp.org/rdf/schema#authoredBy> <https://dblp.org/pid/06/2290> . ?answer <https://dblp.org/rdf/schema#authoredBy> <https://dblp.org/rec/journals/eor/GamacheGC05> } ¡ . ?answer <https://dblp.org/rdf/schema#authoredBy> <https://dblp.org/pid/h/ManuelVHermenegildo> } ¡ <https://dblp.org/rdf/schema#primaryAffiliation> } ¡ } ¡ } ¡ } ¡ } ¡ ?answer != != != != != != != != != != != != != != != != !=\n",
            "Sample 4:\n",
            " ¿ SELECT DISTINCT ?answer WHERE { <https://dblp.org/rec/conf/ofc/BitachonGBEL30> <https://dblp.org/rdf/schema#authoredBy> <https://dblp.org/pid/185/7015> . <https://dblp.org/rec/conf/smc/BristowB17> <https://dblp.org/rdf/schema#publishedIn> ?answer } ¡ <https://dblp.org/rdf/schema#primaryAffiliation> ?answer } ¡ . ?answer <https://dblp.org/rdf/schema#publishedIn> ?answer } ¡ } ¡ } ¡ . ?answer } ¡ } ¡ <https://dblp.org/rdf/schema#primaryAffiliation> ?answer } ¡ != != != != != != != != != != !=\n",
            "Sample 5:\n",
            " ¿ SELECT DISTINCT ?firstanswer ?secondanswer WHERE { <https://dblp.org/rec/journals/jgo/VinkoN07> <https://dblp.org/rdf/schema#authoredBy> ?firstanswer . ?secondanswer <https://dblp.org/rdf/schema#authoredBy> ?firstanswer FILTER ( ?secondanswer != <https://dblp.org/rec/journals/jgo/VinkoN07> ) } ¡ } ¡ != != != != != != != != != != != != != != != != != != != != != != != != != !=\n",
            "Sample 6:\n",
            " ¿ SELECT DISTINCT ( COUNT( ?answer ) AS ?count ) WHERE { <https://dblp.org/rec/journals/jct/SalesM98> <https://dblp.org/rdf/schema#authoredBy> ?x . ?answer <https://dblp.org/rdf/schema#authoredBy> ?x . ?answer <https://dblp.org/rdf/schema#publishedIn> 'J. Comb. Theory, Ser. B' } ¡ 'Scalable != != != != != != != != != != != != != != != != != != != !=\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "acc1, acc3, mhd = test_model(n_test, m, word_encoder, word_decoder, test_data)\n",
        "print('Accuracy@1 on test: {}%'.format(acc1 * 100))\n",
        "print('Accuracy@3 on test: {}%'.format(acc3 * 100))\n",
        "print('Mean Hamming distance on test: {}'.format(mhd))"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UMhNDPhv1aAq",
        "outputId": "b22ba2ad-e215-48fa-ca29-ee5889f136a3"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Sample:  0\n",
            "Sample:  20\n",
            "Sample:  40\n",
            "Sample:  60\n",
            "Sample:  80\n",
            "Sample:  100\n",
            "Sample:  120\n",
            "Sample:  140\n",
            "Sample:  160\n",
            "Sample:  180\n",
            "Accuracy@1 on test: 45.405%\n",
            "Accuracy@3 on test: 54.595000000000006%\n",
            "Mean Hamming distance on test: 1.08649\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Qualitative testing"
      ],
      "metadata": {
        "id": "LTO0NNIhJukg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "m.load(\"/content/drive/MyDrive/transformer-pretrained-final.pth\", optimizer)"
      ],
      "metadata": {
        "id": "Gj6oO--sIPjk"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "q = \"What are the papers written by the person Hideaki Takeda?\"\n",
        "entities = [\"<https://dblp.org/pid/27/4034-1>\"]\n",
        "q = format_question(q)\n",
        "print(q)\n",
        "q = process_question(entities, q)\n",
        "print(q)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "F4gnZPdEBzjq",
        "outputId": "7b3e53ad-b61d-42ab-f316-6ae9f8c30406"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "what are the papers written by the person Hideaki Takeda\n",
            "what are the papers written by the person <https://dblp.org/pid/27/4034-1>\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "n_tokens = 9\n",
        "in_t = torch.tensor(word_encoder(q), dtype=torch.long)\n",
        "padded_text = F.pad(in_t, (0, MAX_IN_BLOCK - len(in_t)), value=1)\n",
        "word_decoder(m.generate(padded_text.unsqueeze(0).to(device), word_encoder('¿'), n_tokens)[0].tolist()[1:])"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "YLTUNw8FEZFB",
        "outputId": "738d7d04-4aa4-4915-ca72-3636618d03df"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'SELECT DISTINCT ?answer WHERE { ?answer <https://dblp.org/rdf/schema#authoredBy> <https://dblp.org/pid/26/4711> }'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 38
        }
      ]
    }
  ]
}